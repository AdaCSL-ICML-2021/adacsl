{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\ntorch.manual_seed(42)\nnp.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from torch.hub import load_state_dict_from_url\n__all__ = ['ResNet', 'resnet18']\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',}\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return out\nclass Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return out\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x):\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        x = nn.functional.softmax(x, 1)\n\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n\n\ndef _resnet(arch, block, layers, pretrained, progress, **kwargs):\n    model = ResNet(block, layers, **kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls[arch],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n    return model\ndef resnet18(pretrained=False, progress=True, **kwargs):\n    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"\"\nTEST = ''\nTRAIN = ''\nVAL =''\n\ndef data_transforms(phase):\n    if phase == TRAIN:\n        transform = transforms.Compose([\n            transforms.Resize((50, 50)),\n            transforms.ToTensor(),\n#             transforms.Normalize([1.2201, 0.4040, 0.7327], [0.3852, 0.5240, 0.5857]),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n        \n    if phase == VAL:\n        transform = transforms.Compose([\n            transforms.Resize((50, 50)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n    \n    if phase == TEST:\n        transform = transforms.Compose([\n            transforms.Resize((50, 50)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])        \n        \n    return transform\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms(x)) \n                  for x in [TRAIN, VAL, TEST]}\ndataloaders = {TRAIN: torch.utils.data.DataLoader(image_datasets[TRAIN], batch_size = 64, shuffle=True), \n               VAL: torch.utils.data.DataLoader(image_datasets[VAL], batch_size = 256, shuffle=True), \n               TEST: torch.utils.data.DataLoader(image_datasets[TEST], batch_size = 256, shuffle=True)}\ndataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\nclasses = image_datasets[TRAIN].classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes, counts = np.unique(dataloaders[TRAIN].dataset.targets, return_counts=True)\nprint(classes)\nprint(counts)\nclass_inverse_priors = {c: 1 / (count/len(dataloaders[TRAIN].dataset.targets)) for c, count in zip(list(classes), counts)}\nprint(class_inverse_priors)\ncost_matrix = pd.DataFrame(data = [[0, class_inverse_priors[0]], [5 * class_inverse_priors[1], 0]], index=['actual negative (0)', 'actual positive (1)'], columns=['predict negative (0)', 'predict positive (1)'])\nprint(cost_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cost(preds, labels, cost_matrix):\n    preds = preds.detach().cpu().numpy()\n    labels = labels.detach().cpu().numpy()\n    \n    cost = []\n    for y_hat, y_actual in zip(preds, labels):\n        cost.append(get_cost_from_matrix(y_hat=y_hat, \\\n                                         y_actual=y_actual,\\\n                                         cost_matrix=cost_matrix))\n        \n    return sum(cost)\n\ndef get_cost_from_matrix(y_hat, y_actual, cost_matrix):    \n    if y_actual == y_hat == 0:\n        return cost_matrix.loc['actual negative (0)', 'predict negative (0)']\n\n    if y_actual == y_hat == 1:\n        return cost_matrix.loc['actual positive (1)', 'predict positive (1)']\n\n    if (y_actual == 1) & (y_hat == 0):\n        return cost_matrix.loc['actual positive (1)', 'predict negative (0)']\n\n    if (y_actual == 0) & (y_hat == 1):\n        return cost_matrix.loc['actual negative (0)', 'predict positive (1)']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_cross_entropy(output, target, cost_matrix):\n    \n    output = torch.clamp(output,min=1e-5,max=1-1e-5)\n    \n    c10 = cost_matrix.loc['actual positive (1)', 'predict negative (0)']\n    c01 = cost_matrix.loc['actual negative (0)', 'predict positive (1)']\n    \n    loss = c10 * (target * torch.log(output[:, 1])) +\\\n    c01 * ((1 - target) * torch.log(1 - output[:, 1]))\n               \n    return torch.neg(torch.mean(loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion, epoch, num_epochs):\n    \n    print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n    print(\"=\" * 10)\n    \n    model.train()\n\n    running_loss = 0.0\n    running_corrects = 0\n    total_cost = 0.0\n    \n    for data in dataloader:\n        \n        inputs, labels = data\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        cost = get_cost(preds, labels, cost_matrix)        \n        loss = criterion(outputs, labels, cost_matrix)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        total_cost += cost\n        \n    epoch_loss = running_loss / dataset_sizes[TRAIN]\n    epoch_acc = running_corrects.double() / dataset_sizes[TRAIN]\n        \n    print('Train:  {:.4f}: , Accuracy: {:.4f},\\\n    Cost: {:.4f}'.format(epoch_loss, epoch_acc, total_cost))\n    \n    return epoch_loss, epoch_acc, total_cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, dataloader, criterion, phase, epoch, save=True):\n        \n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    total_cost = 0.0\n    \n    with torch.no_grad():\n        for data in dataloaders[phase]:\n        \n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            \n            cost = get_cost(preds, labels, cost_matrix)\n            loss = criterion(outputs, labels, cost_matrix)\n            \n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n            total_cost += cost\n            \n    epoch_loss = running_loss / dataset_sizes[phase]\n    epoch_acc = running_corrects.double() / dataset_sizes[phase]\n    \n    print('{}:  {:.4f}: , Accuracy: {:.4f},\\\n    Cost: {:.4f}'.format(phase, epoch_loss, epoch_acc, total_cost))\n    \n    if save:\n      global best_valid_cost\n      if total_cost < best_valid_cost:\n        best_valid_cost = total_cost\n        print('Saving..')\n        state = {\n            'net': model.state_dict(),\n            'epoch': epoch,\n            'acc': epoch_acc,\n            'cost': total_cost,\n        }\n        if not os.path.isdir('checkpoint'):\n          os.mkdir('checkpoint')\n        torch.save(state, './checkpoint/ckpt.pth')\n\n    \n    return epoch_loss, epoch_acc, total_cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet = resnet18(pretrained=True)\nresnet.fc = nn.Linear(512, 2)\nresnet = resnet.to(device)\n\ncriterion = weighted_cross_entropy\noptimizer = optim.SGD(resnet.parameters(), lr=0.01, weight_decay=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 15\n\nbest_valid_cost = 1e6\n\ntrain_loss, valid_loss, train_acc, valid_acc,train_cost, valid_cost = [], [], [], [], [], []\n\n\nfor epoch in range(num_epochs):\n    tr_epoch_loss, tr_epoch_acc, tr_epoch_cost= train(resnet, \n                                                       dataloaders[TRAIN], \n                                                       optimizer,\n                                                       criterion,\n                                                       epoch,\n                                                       num_epochs)\n\n    train_loss.append(tr_epoch_loss)\n    train_acc.append(tr_epoch_acc)\n    train_cost.append(tr_epoch_cost)\n\n    val_epoch_loss, val_epoch_acc, val_epoch_cost = evaluate(resnet, \n                                                              dataloaders,\n                                                              criterion,\n                                                              VAL,\n                                                              epoch,\n                                                              save=True)\n\n    valid_loss.append(val_epoch_loss)\n    valid_acc.append(val_epoch_acc)\n    valid_cost.append(val_epoch_cost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = torch.load('./checkpoint/ckpt.pth')\nresnet.load_state_dict(checkpoint['net'])\nbest_cost = checkpoint['cost']\ncurrent_epoch = checkpoint['epoch']\n\nprint(best_cost)\nprint(current_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(resnet, dataloaders, criterion, TEST, epoch, save=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({'train loss': train_loss,\n              'train acc': [x.item() for x in train_acc],\n              'train cost': train_cost, \n              'valid loss': valid_loss,\n              'valid acc': [x.item() for x in valid_acc],\n              'valid cost': valid_cost})","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}